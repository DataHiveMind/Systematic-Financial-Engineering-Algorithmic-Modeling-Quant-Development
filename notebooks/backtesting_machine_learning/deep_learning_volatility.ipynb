{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e681e892",
   "metadata": {},
   "source": [
    "# Neural Network Volatility Prediction\n",
    "\n",
    "This notebook demonstrates neural network-driven volatility prediction using LSTM, Transformer, and reinforcement learning, with comparison to traditional econometric models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853135c7",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Set Up Environment\n",
    "\n",
    "Import Python libraries for data manipulation, deep learning (TensorFlow/Keras, PyTorch), and visualization. Set random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a1540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning frameworks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Econometric models\n",
    "from arch import arch_model\n",
    "\n",
    "# Miscellaneous\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74b0b23",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Financial Time-Series Data\n",
    "\n",
    "Load historical asset price and volatility data. Handle missing values, normalize features, and split into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load sample data (replace with your own data source)\n",
    "# For demonstration, we'll use Yahoo Finance data via yfinance\n",
    "import yfinance as yf\n",
    "\n",
    "symbol = 'SPY'\n",
    "data = yf.download(symbol, start='2010-01-01', end='2023-01-01')\n",
    "data = data[['Adj Close']].rename(columns={'Adj Close': 'price'})\n",
    "\n",
    "# Calculate daily returns\n",
    "data['return'] = np.log(data['price'] / data['price'].shift(1))\n",
    "data['volatility'] = data['return'].rolling(window=21).std() * np.sqrt(252)\n",
    "\n",
    "# Drop missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Normalize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data[['price', 'return', 'volatility']] = scaler.fit_transform(data[['price', 'return', 'volatility']])\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_size = int(len(data) * 0.7)\n",
    "val_size = int(len(data) * 0.15)\n",
    "train = data.iloc[:train_size]\n",
    "val = data.iloc[train_size:train_size+val_size]\n",
    "test = data.iloc[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5c106",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering for Volatility Prediction\n",
    "\n",
    "Generate features such as log returns, rolling statistics, and lagged volatility measures to enhance model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    df['return_lag1'] = df['return'].shift(1)\n",
    "    df['return_lag2'] = df['return'].shift(2)\n",
    "    df['volatility_lag1'] = df['volatility'].shift(1)\n",
    "    df['volatility_lag2'] = df['volatility'].shift(2)\n",
    "    df['rolling_mean_5'] = df['return'].rolling(window=5).mean()\n",
    "    df['rolling_std_5'] = df['return'].rolling(window=5).std()\n",
    "    df['rolling_mean_21'] = df['return'].rolling(window=21).mean()\n",
    "    df['rolling_std_21'] = df['return'].rolling(window=21).std()\n",
    "    return df\n",
    "\n",
    "train = add_features(train).dropna()\n",
    "val = add_features(val).dropna()\n",
    "test = add_features(test).dropna()\n",
    "\n",
    "feature_cols = [\n",
    "    'price', 'return', 'return_lag1', 'return_lag2',\n",
    "    'volatility_lag1', 'volatility_lag2',\n",
    "    'rolling_mean_5', 'rolling_std_5',\n",
    "    'rolling_mean_21', 'rolling_std_21'\n",
    "]\n",
    "target_col = 'volatility'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2419c1f8",
   "metadata": {},
   "source": [
    "## 4. Build and Train LSTM Model\n",
    "\n",
    "Define, compile, and train an LSTM neural network for volatility forecasting. Track training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Prepare data for LSTM\n",
    "def create_sequences(X, y, seq_length=21):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        Xs.append(X[i:(i+seq_length)])\n",
    "        ys.append(y[i+seq_length])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "seq_length = 21\n",
    "X_train, y_train = create_sequences(train[feature_cols].values, train[target_col].values, seq_length)\n",
    "X_val, y_val = create_sequences(val[feature_cols].values, val[target_col].values, seq_length)\n",
    "X_test, y_test = create_sequences(test[feature_cols].values, test[target_col].values, seq_length)\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, input_shape=(seq_length, len(feature_cols)), return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3beda0f",
   "metadata": {},
   "source": [
    "## 5. Build and Train Transformer Model\n",
    "\n",
    "Implement a Transformer-based model for time-series volatility prediction. Train and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c91d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = Dense(ff_dim, activation=\"relu\")(res)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "input_layer = Input(shape=(seq_length, len(feature_cols)))\n",
    "x = transformer_encoder(input_layer, head_size=32, num_heads=2, ff_dim=64, dropout=0.1)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "output_layer = Dense(1)(x)\n",
    "\n",
    "transformer_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "transformer_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history_transformer = transformer_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dd843a",
   "metadata": {},
   "source": [
    "## 6. Monte Carlo-Based Reinforcement Learning for Trade Optimization\n",
    "\n",
    "Apply Monte Carlo simulations and reinforcement learning (e.g., DQN or policy gradient) to optimize trading strategies based on predicted volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3114919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple Monte Carlo simulation for trading strategy based on predicted volatility\n",
    "\n",
    "# For demonstration, use LSTM predictions on test set\n",
    "lstm_preds = lstm_model.predict(X_test).flatten()\n",
    "\n",
    "# Simulate a simple volatility-based trading strategy\n",
    "# If predicted volatility is above median, reduce position; else, increase position\n",
    "positions = np.where(lstm_preds > np.median(lstm_preds), 0.5, 1.5)  # leverage factor\n",
    "\n",
    "returns = test[target_col].values[seq_length:] * positions  # pseudo returns\n",
    "cumulative_returns = np.cumprod(1 + returns) - 1\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(cumulative_returns, label='Volatility-Adjusted Strategy')\n",
    "plt.title('Monte Carlo Simulated Trading Strategy')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# For full RL, consider using stable-baselines3 or keras-rl for DQN/Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc154fce",
   "metadata": {},
   "source": [
    "## 7. Implement Traditional Econometric Volatility Models\n",
    "\n",
    "Fit and forecast volatility using models such as GARCH or EWMA for baseline comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b66216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GARCH(1,1) model as a traditional baseline\n",
    "garch_model = arch_model(train['return'], vol='Garch', p=1, q=1)\n",
    "garch_fitted = garch_model.fit(disp='off')\n",
    "\n",
    "# Forecast volatility on test set\n",
    "garch_forecast = garch_fitted.forecast(horizon=len(test), start=train.index[-1])\n",
    "garch_vol = np.sqrt(garch_forecast.variance.values[-1, :])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(test.index[seq_length:], y_test, label='True Volatility')\n",
    "plt.plot(test.index[seq_length:], garch_vol[seq_length:], label='GARCH Forecast')\n",
    "plt.title('GARCH Volatility Forecast vs True')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volatility')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a09c97",
   "metadata": {},
   "source": [
    "## 8. Compare Model Performance and Visualize Results\n",
    "\n",
    "Evaluate and compare the predictive accuracy and trading outcomes of all models. Visualize results with plots and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# LSTM predictions\n",
    "lstm_preds = lstm_model.predict(X_test).flatten()\n",
    "# Transformer predictions\n",
    "transformer_preds = transformer_model.predict(X_test).flatten()\n",
    "# GARCH predictions already computed\n",
    "\n",
    "# Compute MSE\n",
    "mse_lstm = mean_squared_error(y_test, lstm_preds)\n",
    "mse_transformer = mean_squared_error(y_test, transformer_preds)\n",
    "mse_garch = mean_squared_error(y_test, garch_vol[seq_length:])\n",
    "\n",
    "print(f\"LSTM MSE: {mse_lstm:.4f}\")\n",
    "print(f\"Transformer MSE: {mse_transformer:.4f}\")\n",
    "print(f\"GARCH MSE: {mse_garch:.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test.index[seq_length:], y_test, label='True Volatility', color='black')\n",
    "plt.plot(test.index[seq_length:], lstm_preds, label='LSTM', alpha=0.7)\n",
    "plt.plot(test.index[seq_length:], transformer_preds, label='Transformer', alpha=0.7)\n",
    "plt.plot(test.index[seq_length:], garch_vol[seq_length:], label='GARCH', alpha=0.7)\n",
    "plt.title('Volatility Prediction: True vs Models')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volatility')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb9394",
   "metadata": {},
   "source": [
    "---\n",
    "**Conclusion:**  \n",
    "This notebook demonstrated the use of LSTM and Transformer neural networks, Monte Carlo-based reinforcement learning, and traditional econometric models for volatility prediction and trading strategy optimization. The results highlight the strengths and weaknesses of each approach for financial risk assessment."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
